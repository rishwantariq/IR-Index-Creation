# -*- coding: utf-8 -*-
"""17L-4095 - A3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zKrtgWZn-k8T8EOaHUr-fVAn5YTMMPqA

**Information Retrieval - Assignment 3 (Done on Google Colab)**
# Rishwan Tariq 
# 17L - 4095
"""

import os
import glob
from google.colab import drive
drive.mount('/content/drive')
from bs4 import BeautifulSoup
import lxml
import urllib
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from nltk.stem import PorterStemmer

from google.colab import drive
drive.mount('/content/drive')

from collections import defaultdict
inverted_index = defaultdict(list)
posting_list = defaultdict(list)
DOCID = 0

fp=open('/content/drive/MyDrive/corpus/docInfo.txt','w+')
def process_directory(name):
  files = 0
  # process directory 
  main_directory = '/content/drive/MyDrive/corpus/corpus1/'
  directory = main_directory + name
  global DOCID
  for filename in os.listdir(directory):
   # if filename.endswith('.html'):
      fname = os.path.join(directory,filename)
      print('processing document: ', fname)
      with open(fname, 'r', errors = 'ignore') as f:
        if files == 30:
          break
        files+=1
        contents = f.read() 
        if contents.find("<!DOCTYPE"):
          contents = contents[contents.find("<!DOCTYPE"):]
         #read text after the HTTP header
        # parsing the HTML text with beautiful soup
        soup = BeautifulSoup(contents, features="html.parser")

        # remove headers and tags of HTML files in the directory
        for script in soup(["script", "style","template"]):
          script.extract()    # remove elements that are not required

        text = soup.get_text() # position of a term will be checked relative to this text (original text)
        print(text)

         # this will filter out special characters with only alphabets or alphanumeric numbers 
        tokens = tokenize_text(soup.get_text())

        # save original tokens to check for duplicates later
        tokens_original = lower_text(tokens)
        tokens_original = stem_words(tokens_original)

        ### processed text from document
        
        # convert to lowercase
        lower=lower_text(tokens)
        # remove stop words
        text_processed=remove_stopwords(lower)
        # stem words
        text_stemmed=stem_words(text_processed)

        #print(text_stemmed) # final text, stemmed 

        # remove duplicates from the text 
        tokens_final = list(dict.fromkeys(text_stemmed))

        print(tokens_final)
        create_index(DOCID, tokens_final, tokens_original)
        # find position and frequency here as well to use later for each term in each document


        #Task 4
        additional_info(DOCID, name, fname, tokens_final)
        DOCID +=1
        print('\n\n')

def tokenize_text(text):
  Tokens = text.split()
  # only keep words containing alphabets, numbers, or both. No special characters.
  #for idx, word in enumerate(Tokens):
    #if not (word.encode().isalpha()):
     # del Tokens[idx]
  
  Tokens = [t for t in Tokens if t.encode().isalpha()]
  return Tokens

def lower_text(Tokens):
  for idx, word in enumerate(Tokens):
    Tokens[idx]=word.lower()
  return Tokens

def remove_stopwords(Tokens):
  sw = stopwords.words("english")
  filtered_words = [w for w in Tokens if not w in sw]    
  return filtered_words

def stem_words(Tokens):
  # use porter stemmer to stem words 
  ps = PorterStemmer()
  for idx, word in enumerate(Tokens):
    Tokens[idx] = ps.stem(word)
        
  return Tokens
  
def additional_info(docID, sub_directory, docName, Tokens):
  # now store the data into file and parse URLS
  document_length = len(Tokens) 
  text_write = str(docID) + ',' + sub_directory + docName + ',' + str(document_length) + ','+ 'magnitude'
  # store all the text in the file as raw text 
  fp.write(text_write + '\n\n')

"""Task 2 - Inverted Index in memory – Step 1 & Step 2

"""

def create_index(id, Tokens, Tokens_original):
  for idx, word in enumerate(Tokens):
    freq = Tokens_original.count(word) # number of occurences of a 'word' in the document 
    if freq == 0:
      continue
    pos = find_position(Tokens_original, word)
    inverted_index[word].append([id,freq, pos])
  #print(inverted_index)

def create_posting(dir):
  global inverted_index
  inverted_index = dict(sorted(inverted_index.items()))
  name_posting = '/content/drive/MyDrive/corpus/index_postings' + dir + '.txt'
  fp1=open(name_posting,'w+', encoding='utf-8')
  name_index = '/content/drive/MyDrive/corpus/index_terms' + dir + '.txt'
  fp2=open(name_index,'w+', encoding='utf-8')

  prev_ID = 0
  doc_id = 0
  prev_pos = 0
  pos = 0

  # find doc frequency for a term
  for key, value in inverted_index.items():

    # check the df - number of documents containing the key
    word, df = key, len([item for item in value if item])
    d_values = inverted_index.get(key)
    text_write = str(df)
    for idx, value in enumerate(d_values):
      print(key)
      print(value)
      doc_id = value[0]
      freq = value[1]
      positions_list = value[2]
      doc_id -= prev_ID
      prev_ID = value[0]
      text_write += ',' + str(doc_id) + ',' + str(freq)
      

      for idx, positions in enumerate(positions_list):
        pos=positions
        pos-=prev_pos
    
        text_write += ',' + str(pos) 
        prev_pos = positions

      prev_pos = 0
    prev_ID = 0

    posting_file = key + ': ' + str(fp1.tell()) + '\n'
    text_write+='\n'
    fp1.write(text_write)
    fp2.write(posting_file)


    
    print('doc freq: ', df)
    print('DOCID: ', value[0])
    print('FREq: ', value[1])
    print('pos: ', value[2])
    print('final text: ', text_write)
    print('next\n\n')
  



def find_position(Tokens_original, word): # tokens original is the words as they are found in document all characters
  positions = [i for i, x in enumerate(Tokens_original) if x == word]
  return positions

process_directory('1')
create_posting('1')
posting_list = defaultdict(list)
inverted_index = defaultdict(list)

process_directory('2')
create_posting('2')
posting_list = defaultdict(list)
inverted_index = defaultdict(list)

process_directory('3')
posting_list = defaultdict(list)
create_posting('3')

"""Merge - Task 3"""

fp1 = open('/content/drive/MyDrive/corpus/index_terms2.txt','r',encoding='utf-8')

fpt = [open(f'/content/drive/MyDrive/corpus/index_terms{i+1}.txt','r',encoding='utf-8') for i in range(3)]
fpp = [open(f'/content/drive/MyDrive/corpus/index_postings{i+1}.txt','r',encoding='utf-8') for i in range(3)]

f_postings = open('/content/drive/MyDrive/corpus/index_postings.txt','w+',encoding='utf-8')
f_index = open('/content/drive/MyDrive/corpus/index_terms.txt','w+',encoding='utf-8')



def create_merged():
  word_1,pos1 = read_file(fpt[0])
  word_2,pos2 = read_file(fpt[1])
  word_3,pos3 = read_file(fpt[2])

  p_1 = read_plist(fpp[0],pos1)
  p_2 = read_plist(fpp[1],pos2)
  p_3 = read_plist(fpp[2],pos3)

  while 1:
    if word_1 == word_2 == word_3:
      # merge all and write to file_postings
      words = []
      words.append(p_1)
      words.append(p_2)
      words.append(p_3)
      merged = merge_plist(words)
      write_index(merged,word_1)
      # get 3 new words read_file
      word_1,pos1 = read_file(fpt[0])
      word_2,pos2 = read_file(fpt[1])
      word_3,pos3 = read_file(fpt[2])

      p_1 = read_plist(fpp[0],pos1)
      p_2 = read_plist(fpp[1],pos2)
      p_3 = read_plist(fpp[2],pos3)

    elif word_1 == word_2:
      words = []
      words.append(p_1)
      words.append(p_2)
      merged = merge_plist(words)
      if word_1 < word_3:
        write_index(merged,word_1)
      # get 2 new words read_file
        word_1,pos1 = read_file(fpt[0])
        word_2,pos2 = read_file(fpt[1])

        p_1 = read_plist(fpp[0],pos1)
        p_2 = read_plist(fpp[1],pos2)

      else:
        write_index(p_3,word_3)
        word_3,pos3 = read_file(fpt[2])
        p_3 = read_plist(fpp[2],pos3)

    elif word_1 == word_3:
      words = []
      words.append(p_1)
      words.append(p_3)
      merged = merge_plist(words)
      if word_1 < word_2:
        write_index(merged,word_1)
      # get 2 new words read_file
        word_1,pos1 = read_file(fpt[0])
        word_3,pos3 = read_file(fpt[2])

        p_1 = read_plist(fpp[0],pos1)
        p_3 = read_plist(fpp[2],pos3)

      else:
        write_index(p_2,word_2)
        word_2,pos2 = read_file(fpt[1])
        p_2 = read_plist(fpp[1],pos2)

    elif word_2 == word_3:
      words = []
      words.append(p_2)
      words.append(p_3)
      merged = merge_plist(words)
      if word_2 < word_1:
        write_index(merged,word_1)
      # get 2 new words read_file
        word_2,pos2 = read_file(fpt[1])
        word_3,pos3 = read_file(fpt[2])

        p_2 = read_plist(fpp[1],pos2)
        p_3 = read_plist(fpp[2],pos3)

      else:
        write_index(p_1,word_1)
        word_1,pos1 = read_file(fpt[0])
        p_1 = read_plist(fpp[0],pos1)

    else:
      if word_1 < word_2 and word_1 < word_3:
        write_index(p_1,word_1)
        word_1,pos1 = read_file(fpt[0])
        p_1 = read_plist(fpp[0],pos1)
      
      elif word_2 < word_3 and word_2 < word_1:
        write_index(p_2,word_2)
        word_2,pos2 = read_file(fpt[1])
        p_2 = read_plist(fpp[1],pos2)
      
      elif word_3 < word_1 and word_3 < word_2:
        write_index(p_3,word_3)
        word_3,pos3 = read_file(fpt[2])
        p_3 = read_plist(fpp[2],pos3)
        

def write_index(merged,word):
  print('writing to index...')
  byte = f_postings.tell()
  term = word + ' ' + str(byte) + '\n'
  f_index.write(term)
  print('written to index',term)
  for t in merged:
    write_text = ' '.join([str(i) for i in merged])
  f_postings.write(write_text + '\n')
  print('written to posting')


def min_word(word1,word2,word3):
  min = min(word1,word2,word3)
  if min == word1:
    return 1
  if min == word2:
    return 2
  if min == word3:
    return 3

def read_file(fp1):
  line = fp1.readline()
  word_byte = line.split(':')
  print(word_byte)
  return word_byte[0], int(word_byte[1])

def read_plist(fp1,pos):
  fp1.seek(int(pos),0)
  line = fp1.readline()
  line = line.split(',')
  for i in range(0, len(line)):
    line[i] = int(line[i])
  
  print('read plist returns: ', line)
  return line

def merge_plist(plist):
  return_plist = []
  to_merge = len(plist)
  df = 0
  for idx, p in enumerate(plist):
    for i,x in enumerate(p):
      df+=x
      break
  return_plist.append(df)
  
  for idx, p in enumerate(plist):
    for i,x in enumerate(p):
      if i == 0:
        continue
      return_plist.append(x)

  print(return_plist)
  print('returning from merging...')
  return return_plist

create_merged()

"""Task - 5 """

results = []
result = []

tokens = tokenize_text("Hello, abc I # $ @ am Aaron")
tokens = lower_text(tokens)
tokens = remove_stopwords(tokens)
tokens = stem_words(tokens)
print('tokenized query after processing: ', tokens)

fp_pl = open('/content/drive/MyDrive/corpus/index_postings.txt','r',encoding='utf-8')
fp_it = open('/content/drive/MyDrive/corpus/index_terms.txt','r',encoding='utf-8') 
def read_file2(fp1):
  line = fp1.readline()
  word_byte = line.split(' ')
  #print(word_byte)
  return word_byte[0], int(word_byte[1])

def read_plist(fp1,pos):
  fp1.seek(int(pos),0)
  line = fp1.readline()
  line = line.split(' ')
  for i in range(0, len(line)):
    line[i] = int(line[i])
  return line

results = []
def find_document(query):
  for idx, q_term in enumerate(query):
    fp_it.seek(0,0)
    while (fp_it != ""):
     term , pos = read_file2(fp_it)
     if term == q_term:
       print('term: ', q_term)
       print('pos of posting list: ', pos)
       result.append(read_plist(fp_pl,pos))
       break
    if term != q_term:
      print('term not found!') 
    
  print('posting list: ', result)
  return result

doc_id = []
def extract_id(p):
  d_id = 0
  for idx, l in enumerate(p):
    length = len(l)
    i = 0
    df = l[0]
    i+=1
    while i < length:
      if i == 1:
        d_id = l[i]
      else:
        d_id+=l[i]
      
      doc_id.append(d_id)
      i += 1
      tf = l[i]
      i+=1
      x = 0
      while x < tf:
        x+=1
        i+=1
  print(doc_id)
  return list(set(doc_id))

def get_details(doc):
  fp_di = open('/content/drive/MyDrive/corpus/docInfo.txt','r',encoding='utf-8') 
  for line in fp_di:
    d = line.split(",")
    #print('d 0 is : ', d[0])
    #print('doc id is: ', doc)
    if d[0] is not '\n' and int(d[0]) is doc:
      print(d[1])  


results = find_document(tokens)
results = extract_id(results)
length = len(results)
k = 0
#print('results: ', results)
# Iterating using while loop
while k < length:
  get_details(results[k])
  k+= 1

print('\n')